# ğŸ“˜ Analyse Syntaxique NLP

## ğŸš€ Description
Ce projet est une **analyse syntaxique multilingue** (franÃ§ais et corÃ©en) utilisant **SpaCy** pour l'extraction des dÃ©pendances syntaxiques et leur **visualisation**.

ğŸ“Œ **Langues supportÃ©es** :
- ğŸ‡«ğŸ‡· FranÃ§ais (`fr_core_news_md`)
- ğŸ‡°ğŸ‡· CorÃ©en (`ko_core_news_sm`)

ğŸ“Œ **FonctionnalitÃ©s** :
- Analyse morphologique et syntaxique avec SpaCy
- Visualisation des dÃ©pendances syntaxiques
- Automatisation du traitement
- Sauvegarde des rÃ©sultats en fichier JSON/CSV

---

## ğŸ› ï¸ Installation
### 1ï¸âƒ£ **Cloner le dÃ©pÃ´t**
```bash
git clone git@github.com:MarcelALng/analyse-syntaxique-nlp.git
cd analyse-syntaxique-nlp
```

### 2ï¸âƒ£ **CrÃ©er un environnement virtuel et installer les dÃ©pendances**
```bash
python -m venv venv
source venv/bin/activate  # macOS/Linux
# Pour Windows : venv\Scripts\activate
pip install -r requirements.txt
```

### 3ï¸âƒ£ **TÃ©lÃ©charger les modÃ¨les SpaCy**
```bash
python -m spacy download fr_core_news_md
python -m spacy download ko_core_news_sm
```

---

## ğŸš€ **Utilisation**

### 1ï¸âƒ£ **ExÃ©cuter le script dâ€™analyse syntaxique**
```bash
python analyse.py
```

### 2ï¸âƒ£ **Visualiser les dÃ©pendances syntaxiques**
Le script gÃ©nÃ¨re des visualisations interactives avec **displaCy**. Pour les voir :
```bash
python analyse.py --visualisation
```

### 3ï¸âƒ£ **Sauvegarder les rÃ©sultats**
Les rÃ©sultats seront stockÃ©s dans `output/` sous format JSON ou CSV.

---

## ğŸ“Š **Exemple de RÃ©sultat**
Extrait dâ€™analyse syntaxique :
```json
{
  "phrase": "Le chat noir dort sur le canapÃ©.",
  "tokens": [
    {"text": "Le", "pos": "DET"},
    {"text": "chat", "pos": "NOUN"},
    {"text": "noir", "pos": "ADJ"},
    {"text": "dort", "pos": "VERB"},
    {"text": "sur", "pos": "ADP"},
    {"text": "le", "pos": "DET"},
    {"text": "canapÃ©", "pos": "NOUN"},
    {"text": ".", "pos": "PUNCT"}
  ]
}
```

---

## ğŸ—ï¸ **Structure du Projet**
```bash
ğŸ“‚ analyse-syntaxique-nlp/
â”œâ”€â”€ ğŸ“‚ data/                # DonnÃ©es d'entrÃ©e
â”œâ”€â”€ ğŸ“‚ output/              # RÃ©sultats analysÃ©s (JSON/CSV)
â”œâ”€â”€ ğŸ“‚ notebooks/           # Notebooks Jupyter
â”œâ”€â”€ ğŸ“œ analyse.py           # Script principal
â”œâ”€â”€ ğŸ“œ requirements.txt     # DÃ©pendances Python
â”œâ”€â”€ ğŸ“œ README.md            # Documentation
```

---

## ğŸ¤ **Contribuer**
Si vous souhaitez contribuer, ouvrez une issue ou soumettez une PR sur GitHub !

---

## ğŸ“„ **Licence**
Ce projet est sous licence **MIT**. Voir le fichier `LICENSE` pour plus dâ€™informations.

---

## ğŸ“§ **Contact**
ğŸ‘¤ **Marcel Albert NGUYEN**  
âœ‰ï¸ [Email](mailto:marcelalbert@example.com)  
ğŸ”— [LinkedIn](https://www.linkedin.com/in/marcelalbert)


Analyse_Syntaxique_NLP
==============================

Analyst syntaxique multilingue (franÃ§ais, corÃ©en) avec SpaCy et visualisation des dÃ©pendances.

Project Organization
------------

    â”œâ”€â”€ LICENSE
    â”œâ”€â”€ Makefile           <- Makefile with commands like `make data` or `make train`
    â”œâ”€â”€ README.md          <- The top-level README for developers using this project.
    â”œâ”€â”€ data
    â”‚Â Â  â”œâ”€â”€ external       <- Data from third party sources.
    â”‚Â Â  â”œâ”€â”€ interim        <- Intermediate data that has been transformed.
    â”‚Â Â  â”œâ”€â”€ processed      <- The final, canonical data sets for modeling.
    â”‚Â Â  â””â”€â”€ raw            <- The original, immutable data dump.
    â”‚
    â”œâ”€â”€ docs               <- A default Sphinx project; see sphinx-doc.org for details
    â”‚
    â”œâ”€â”€ models             <- Trained and serialized models, model predictions, or model summaries
    â”‚
    â”œâ”€â”€ notebooks          <- Jupyter notebooks. Naming convention is a number (for ordering),
    â”‚                         the creator's initials, and a short `-` delimited description, e.g.
    â”‚                         `1.0-jqp-initial-data-exploration`.
    â”‚
    â”œâ”€â”€ references         <- Data dictionaries, manuals, and all other explanatory materials.
    â”‚
    â”œâ”€â”€ reports            <- Generated analysis as HTML, PDF, LaTeX, etc.
    â”‚Â Â  â””â”€â”€ figures        <- Generated graphics and figures to be used in reporting
    â”‚
    â”œâ”€â”€ requirements.txt   <- The requirements file for reproducing the analysis environment, e.g.
    â”‚                         generated with `pip freeze > requirements.txt`
    â”‚
    â”œâ”€â”€ setup.py           <- makes project pip installable (pip install -e .) so src can be imported
    â”œâ”€â”€ src                <- Source code for use in this project.
    â”‚Â Â  â”œâ”€â”€ __init__.py    <- Makes src a Python module
    â”‚   â”‚
    â”‚Â Â  â”œâ”€â”€ data           <- Scripts to download or generate data
    â”‚Â Â  â”‚Â Â  â””â”€â”€ make_dataset.py
    â”‚   â”‚
    â”‚Â Â  â”œâ”€â”€ features       <- Scripts to turn raw data into features for modeling
    â”‚Â Â  â”‚Â Â  â””â”€â”€ build_features.py
    â”‚   â”‚
    â”‚Â Â  â”œâ”€â”€ models         <- Scripts to train models and then use trained models to make
    â”‚   â”‚   â”‚                 predictions
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ predict_model.py
    â”‚Â Â  â”‚Â Â  â””â”€â”€ train_model.py
    â”‚   â”‚
    â”‚Â Â  â””â”€â”€ visualization  <- Scripts to create exploratory and results oriented visualizations
    â”‚Â Â      â””â”€â”€ visualize.py
    â”‚
    â””â”€â”€ tox.ini            <- tox file with settings for running tox; see tox.readthedocs.io


--------

<p><small>Project based on the <a target="_blank" href="https://drivendata.github.io/cookiecutter-data-science/">cookiecutter data science project template</a>. #cookiecutterdatascience</small></p>
